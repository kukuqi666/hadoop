#!/bin/bash

# 定义颜色变量
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[1;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# 清屏
clear

#文心一言
echo "文心一言" 
echo -e "${BLUE}====================================================${NC}"
echo
curl -s https://api.vvhan.com/api/ian/rand
echo
echo
# 美化输出：打印盒子框架和彩色信息
echo -e "${BLUE}====================================================${NC}"
echo -e "${GREEN}                   Hadoop 安装脚本                 ${NC}"
echo -e "${BLUE}====================================================${NC}"
echo -e "${CYAN}作者   ：${NC}${YELLOW} 琪大大${NC}"
echo -e "${CYAN}QID    ：${NC}${YELLOW} wnfng66${NC}"
echo -e "${CYAN}WeChat ：${NC}${YELLOW} kukuqi6666${NC}"
echo -e "${CYAN}Email  ：${NC}${YELLOW} kukuqi666@gmail.com${NC}"
echo -e "${CYAN}Github ：${NC}${YELLOW} https://github.com/kukuqi666${NC}"
echo -e "${BLUE}====================================================${NC}"

# 打印 Hadoop 艺术字（使用颜色美化）
echo -e "${MAGENTA}"
echo " _   _           _"    
echo "| | | | __ _  __| |  ___   ___  _ __"
echo "| |_| |/ _\` |/ _\` |/ _ \ / _ \| '_ \\"
echo "|  _  | (_| | (_| | (_) | (_) | |_) |"
echo "|_| |_|\__,_|\__,_|\___/ \___/| .__/"
echo "                              |_|"
echo -e "${NC}"
echo
echo -e "${BLUE}====================================================${NC}"
echo -e "${RED}注意：${NC}${GREEN}本脚本仅适用于 CentOS 7，并需要以 root 权限运行。${NC}"
echo -e "${YELLOW}该脚本将下载并安装 Hadoop，使用时请确保网络正常连接。${NC}"
echo
echo -e "${GREEN}按下回车键以继续安装 Hadoop，按 'q' 退出。${NC}"

# 读取用户按键
while true; do
    read -n 1 -s key  # 读取一个按键，-n 1 表示只读一个字符，-s 表示不显示输入
    if [[ "$key" == "" ]]; then
        echo "继续执行安装..."
        break  # 按下回车键继续
    elif [[ "$key" == "q" ]]; then
        echo "已退出安装。"
        exit 0  # 按下 'q' 退出
    else
        echo "输入错误，请按回车键继续或按 'q' 退出。"  # 其他输入提示错误
    fi
done

# 检查是否以 root 用户运行
if [ "$(id -u)" -ne 0 ]; then
  echo "别忘了请以 root 用户运行此脚本哦。"
  exit 1
fi

# 更新系统并安装必要的依赖项
echo "更新系统并安装依赖项..."
yum update -y
yum install -y wget curl vim openssh-server openssh-clients rsync java-11-openjdk-devel

# 清理之前的 SSH 免密登录配置
echo "清理之前的 SSH 免密登录配置..."
rm -f ~/.ssh/id_rsa ~/.ssh/id_rsa.pub
sed -i '/^ssh-rsa/d' ~/.ssh/authorized_keys

# 删除之前安装的 Hadoop 目录
echo "清理之前安装的 Hadoop..."
rm -rf /usr/local/hadoop /usr/local/hadoop-*

# 删除之前的 Hadoop 环境变量配置
rm -f /etc/profile.d/hadoop.sh

# 删除 Hadoop 数据目录
echo "删除之前的 Hadoop 数据目录..."
rm -rf /usr/local/hadoop/hadoop_data

# 创建 hadoop 用户并设置默认密码
HADOOP_USER="hadoop"
HADOOP_PASS="123456"

if id "$HADOOP_USER" &>/dev/null; then
    echo "用户 $HADOOP_USER 已存在，跳过创建步骤。"
else
    echo "创建 Hadoop 用户 $HADOOP_USER..."
    useradd -m $HADOOP_USER
    echo "$HADOOP_USER:$HADOOP_PASS" | chpasswd
    echo "设置默认密码为 $HADOOP_PASS"
    # 给 hadoop 用户添加 sudo 权限
    usermod -aG wheel $HADOOP_USER
    echo "用户 $HADOOP_USER 创建成功，默认密码为 $HADOOP_PASS"
fi

# 启动并设置 SSH 服务
echo "启动并设置 SSH 服务..."
systemctl enable sshd
systemctl start sshd

# 配置 JAVA_HOME
echo "配置 JAVA_HOME..."
JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
echo "export JAVA_HOME=$JAVA_HOME" >> /etc/profile.d/hadoop.sh
echo "export PATH=\$PATH:\$JAVA_HOME/bin" >> /etc/profile.d/hadoop.sh
source /etc/profile.d/hadoop.sh

# 验证 Java 安装
echo "验证 Java 安装..."
java -version

# 定义可用的 Hadoop 版本号
declare -A HADOOP_VERSIONS=(
  ["1"]="3.4.0"
  ["2"]="3.3.6"
  ["3"]="3.3.5"
  ["4"]="3.2.4"
  ["5"]="2.10.2"
)

# 显示 Hadoop 版本号选择菜单
echo "请选择要安装的 Hadoop 版本号："
echo "1) Hadoop 3.4.0"
echo "2) Hadoop 3.3.6"
echo "3) Hadoop 3.3.5"
echo "4) Hadoop 3.2.4"
echo "5) Hadoop 2.10.2"
read -p "请输入对应的序号（1/2/3/4/5）： " version_choice

# 根据用户选择设置 Hadoop 版本号
HADOOP_VERSION="${HADOOP_VERSIONS[$version_choice]}"
if [ -z "$HADOOP_VERSION" ]; then
  echo "无效的选择，请重试。"
  exit 1
fi

echo "您选择了 Hadoop 版本：$HADOOP_VERSION"

# 设置下载 URL 和安装路径
HADOOP_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz"
INSTALL_DIR="/usr/local/hadoop"

# 创建 Hadoop 目标目录
mkdir -p /usr/local/hadoop

echo "下载 Hadoop $HADOOP_VERSION..."
wget -c $HADOOP_URL -P /tmp

echo "解压 Hadoop 到 /usr/local/hadoop..."
tar -xzvf /tmp/hadoop-$HADOOP_VERSION.tar.gz -C /usr/local/hadoop --strip-components=1

# 配置环境变量
echo "配置 Hadoop 环境变量..."
HADOOP_ENV_FILE="/etc/profile.d/hadoop.sh"

# 检查文件是否存在，如果不存在则创建文件
if [ ! -f $HADOOP_ENV_FILE ]; then
    touch $HADOOP_ENV_FILE
fi

cat <<EOL > $HADOOP_ENV_FILE
export HADOOP_HOME=$INSTALL_DIR
export HADOOP_INSTALL=\$HADOOP_HOME
export HADOOP_MAPRED_HOME=\$HADOOP_HOME
export HADOOP_COMMON_HOME=\$HADOOP_HOME
export HADOOP_HDFS_HOME=\$HADOOP_HOME
export YARN_HOME=\$HADOOP_HOME
export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin
export JAVA_HOME=$JAVA_HOME
EOL

# 使环境变量立即生效
source $HADOOP_ENV_FILE

echo "Hadoop 环境变量配置完成！"

# 创建 Hadoop 数据目录
HADOOP_DATA_DIR="$INSTALL_DIR/hadoop_data"
mkdir -p $HADOOP_DATA_DIR/hdfs/namenode
mkdir -p $HADOOP_DATA_DIR/hdfs/datanode

# 更新 core-site.xml
echo "更新 core-site.xml..."
cat <<EOL > $HADOOP_HOME/etc/hadoop/core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
EOL

# 更新 hdfs-site.xml
echo "更新 hdfs-site.xml..."
cat <<EOL > $HADOOP_HOME/etc/hadoop/hdfs-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://$HADOOP_DATA_DIR/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://$HADOOP_DATA_DIR/hdfs/datanode</value>
    </property>
</configuration>
EOL

echo "Hadoop 配置文件更新完成！"

# 配置 SSH 免密码登录
echo "设置 SSH 免密码登录..."
su - $HADOOP_USER -c "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa"
su - $HADOOP_USER -c "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
su - $HADOOP_USER -c "chmod 600 ~/.ssh/authorized_keys"
su - $HADOOP_USER -c "chmod 700 ~/.ssh"

# 确保 PubkeyAuthentication 启用并重启 sshd
echo "配置 SSH 以确保免密码登录..."
SSHD_CONFIG="/etc/ssh/sshd_config"
if ! grep -q "^PubkeyAuthentication yes" $SSHD_CONFIG; then
  echo "PubkeyAuthentication yes" >> $SSHD_CONFIG
fi
if ! grep -q "^AuthorizedKeysFile .ssh/authorized_keys" $SSHD_CONFIG; then
  echo "AuthorizedKeysFile .ssh/authorized_keys" >> $SSHD_CONFIG
fi
systemctl restart sshd

# 测试 SSH 免密码登录
echo "测试 SSH 免密码登录..."
su - $HADOOP_USER -c "ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no localhost 'echo \"SSH 免密码登录配置成功。\"'" || echo "SSH 免密码登录失败，请检查 SSH 配置。"

# 修复 Hadoop 日志目录权限问题
echo "修复 Hadoop 日志目录权限..."
mkdir -p /usr/local/hadoop/logs
chown -R $HADOOP_USER:$HADOOD_USER /usr/local/hadoop/logs
chmod -R 755 /usr/local/hadoop/logs

# 创建 Hadoop 数据目录
echo "创建 Hadoop 数据目录..."
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
chown -R $HADOOP_USER:$HADOOP_USER /usr/local/hadoop/hadoop_data

# 格式化 HDFS
echo "格式化 HDFS..."
su - $HADOOP_USER -c "hdfs namenode -format -force"

# 启动 Hadoop
echo "启动 Hadoop..."
su - $HADOOP_USER -c "start-dfs.sh"
su - $HADOOP_USER -c "start-yarn.sh"

# 打印防火墙和安装完成信息
echo -e "${BLUE}==========================================================${NC}"
echo
# 检查防火墙状态
if systemctl is-active --quiet firewalld; then
  echo -e "${GREEN}防火墙正在运行，配置防火墙以允许访问 Hadoop Web 界面...${NC}"
  firewall-cmd --permanent --add-port=9870/tcp   # NameNode Web UI
  firewall-cmd --permanent --add-port=8088/tcp   # ResourceManager Web UI
  firewall-cmd --reload                            # 重新加载配置
  echo -e "${GREEN}防火墙配置完成。${NC}"
else
  echo -e "${RED}警告：防火墙未运行，建议启动防火墙以增强安全性。${NC}"
fi

echo

# 获取本地 IP 地址，假设机器有一个 eth0 网络接口
ip_address=$(hostname -I | awk '{print $1}')  # 获取第一个 IP 地址

# 打印安装完成信息
echo -e "${BLUE}=========================================================${NC}"
echo
echo -e "${GREEN}Hadoop 安装和配置完成！尽情玩耍吧老弟！${NC}"
echo 
echo -e "${YELLOW}您可以使用 'jps' 命令检查 Hadoop 进程状态：${NC}"
echo 
echo -e "${BLUE}=========================================================${NC}"
echo -e "${CYAN}您可以通过以下 URL 访问 Hadoop Web 界面：${NC}"
# 用动态获取的IP替换localhost
echo -e "NameNode Web UI: http://${ip_address}:9870/"
echo -e "ResourceManager Web UI: http://${ip_address}:8088/"
echo 
echo -e "${MAGENTA}如果能进行到这一步说明你已经安装成功了，那么${NC}"
echo -e "${MAGENTA}下次启动你就无需运行此脚本了，感谢你的使用！${NC}"
echo -e "${MAGENTA}启动和关闭hadoop直接运行hadoop_server.sh就行了${NC}"
echo -e "${BLUE}=========================================================${NC}"
